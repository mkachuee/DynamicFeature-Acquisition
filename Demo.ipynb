{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Feature Acquisition Using Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "import sklearn.cluster\n",
    "import sklearn.feature_selection\n",
    "import sklearn.ensemble\n",
    "import sklearn.svm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython import embed\n",
    "\n",
    "import src.datasets\n",
    "import src.nn_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#%pdb\n",
    "%matplotlib notebook\n",
    "np.random.seed(1)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "PLOT = False\n",
    "\n",
    "DIR_SUMMARY = './tf_logs'\n",
    "DIR_MODEL = os.popen('mktemp -d').read().rstrip()+'/'\n",
    "os.system('rm -r ' + DIR_SUMMARY)\n",
    "os.system('rm -r ' + DIR_MODEL)\n",
    "os.system('mkdir ' + DIR_MODEL)\n",
    "\n",
    "#synthesized', 'mnist'\n",
    "DATASET = 'synthesized'\n",
    "RANDSEL_RUNS = 1\n",
    "STATIC_ALG = 'MI' #'SVM', 'MI', 'none'\n",
    "N_BITS = 8\n",
    "OPTIMIZER = tf.train.AdamOptimizer\n",
    "LR_BASE_ENC = 0.001\n",
    "LR_BASE = 0.001\n",
    "\n",
    "\n",
    "if DATASET == 'synthesized':\n",
    "    # data generation\n",
    "    opt = {\n",
    "    'n_features' : 32,\n",
    "    'n_clusters' : 16,\n",
    "    'n_clusterpoints' :1000, \n",
    "    'std_clusters' : 0.25,\n",
    "    'cost-aware' : True,\n",
    "    }\n",
    "    # ENCODER\n",
    "    SIZE_LAYERS_ENCODER = [16,10]\n",
    "    ITER_EARLYSTOP_ENC = 3\n",
    "    ITERS_MAX_ENC = 20000\n",
    "    # NN\n",
    "    EXPERIMENT = 'real_mask'\n",
    "    SIZE_HIDDENS = [8,4]# [8,4]\n",
    "    ITER_EARLYSTOP = 20#5\n",
    "    ITERS_MAX = 100000\n",
    "    # MISSING VALUES\n",
    "    MISSING_PORTION = (1.5,1.5)\n",
    "    # load data\n",
    "    ds = src.datasets.Dataset()\n",
    "    ds.load('synthesized', opt)\n",
    "    ds.preprocess(normalization='unity', fe_std_threshold=0.0)\n",
    "    ds_dict = ds.get(order='rand', onehot=True)\n",
    "    dataset_features = ds_dict['features']\n",
    "    dataset_targets = ds_dict['targets']\n",
    "    dataset_mask = ds_dict['mask']    \n",
    "    dataset_costs = ds_dict['costs']\n",
    "    \n",
    "\n",
    "elif DATASET == 'mnist':\n",
    "    # task\n",
    "    opt = {'task':'multires'}\n",
    "    # ENCODER\n",
    "    SIZE_LAYERS_ENCODER = [64,32]\n",
    "    ITER_EARLYSTOP_ENC = 10\n",
    "    ITERS_MAX_ENC = 10000\n",
    "    # NN\n",
    "    EXPERIMENT = 'real_mask'\n",
    "    SIZE_HIDDENS = [16]\n",
    "    ITER_EARLYSTOP = 10\n",
    "    ITERS_MAX = ITERS_MAX_ENC\n",
    "    # MISSING VALUES\n",
    "    MISSING_PORTION = (3.5,1.5) # NEW (1.5,1.5)\n",
    "    # load data\n",
    "    ds = src.datasets.Dataset()\n",
    "    ds.load('mnist', opt)\n",
    "    ds.preprocess(normalization='unity', fe_std_threshold=1.0e-3)\n",
    "    ds_dict = ds.get(order='rand', onehot=True)\n",
    "    dataset_features = ds_dict['features']\n",
    "    dataset_targets = ds_dict['targets']\n",
    "    dataset_mask = ds_dict['mask']\n",
    "    dataset_costs = ds_dict['costs']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_mlp(input_features, \n",
    "            size_hiddens, size_output, \n",
    "            ):\n",
    "    \"\"\"\n",
    "    Network definition\n",
    "    \"\"\"\n",
    "    # create the net input layer\n",
    "    with tf.name_scope('input'):\n",
    "        layer_activations = input_features\n",
    "        \n",
    "    # hidden layers\n",
    "    for size_hid in size_hiddens:\n",
    "        with tf.name_scope('hidden_'+str(size_hid)):\n",
    "            size_before = int(layer_activations.get_shape()[1])\n",
    "            weights = tf.Variable(\n",
    "                tf.truncated_normal(\n",
    "                    [size_before, size_hid],\n",
    "                    stddev=1.0 / math.sqrt(float(size_before))),\n",
    "                name='weights')\n",
    "            biases = tf.Variable(tf.zeros([size_hid])+0.1, name='biases')\n",
    "            layer_activations = tf.nn.relu(\n",
    "                tf.matmul(layer_activations, weights) + biases)\n",
    "            \n",
    "    # output layer\n",
    "    with tf.name_scope('output'):\n",
    "        size_before = int(layer_activations.get_shape()[1])\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [size_before, size_output],\n",
    "                stddev=1.0 / math.sqrt(float(size_before))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([size_output])+0.0, name='biases')\n",
    "        layer_activations = tf.matmul(layer_activations, weights) + biases\n",
    "        output_prediction = layer_activations\n",
    "    \n",
    "    return output_prediction\n",
    "\n",
    "def loss_cross_entropy(preds, labels):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=preds, name='xentropy')\n",
    "    return tf.reduce_mean(cross_entropy, name='xentropy_mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_autoencoder_bin(input_features, size_layers_encoder, \n",
    "                    input_mask=None, algorithm='nomask'):\n",
    "    \"\"\"\n",
    "    Network definition\n",
    "    \"\"\"\n",
    "    # Build the encoding layers\n",
    "    # create the net input layer\n",
    "    with tf.name_scope('AE_input'):\n",
    "        layer_activations = input_features\n",
    "    \n",
    "    # binary layer\n",
    "    with tf.name_scope('AE_input_bin'):\n",
    "        layer_encoder_bin = encode_binary(input_features)\n",
    "        layer_activations = layer_encoder_bin\n",
    "    # hidden encoder layers\n",
    "    enc_weights = []\n",
    "    for size_hid in size_layers_encoder:\n",
    "        with tf.name_scope('AE_hidden_encoder_'+str(size_hid)):\n",
    "            size_before = int(layer_activations.get_shape()[1])\n",
    "            weights = tf.Variable(\n",
    "                tf.truncated_normal(\n",
    "                    [size_before, size_hid],\n",
    "                    stddev=1.0 / math.sqrt(float(size_before))),\n",
    "                name='weights')\n",
    "            biases = tf.Variable(tf.zeros([size_hid])+0.1, name='biases')\n",
    "            layer_activations = tf.nn.relu(#sigmoid(\n",
    "                tf.matmul(layer_activations, weights) + biases)\n",
    "            enc_weights.append(weights)\n",
    "    \n",
    "    # extract a reference to the encoder\n",
    "    encoded = layer_activations\n",
    "    \n",
    "    # hidden decoder layers\n",
    "    for (size_hid,weights_hid) in \\\n",
    "        zip(size_layers_encoder[::-1], enc_weights[::-1]):\n",
    "        with tf.name_scope('AE_hidden_decoder_'+str(size_hid)):\n",
    "            size_before = int(layer_activations.get_shape()[1])\n",
    "            size_after = int(weights_hid.get_shape()[0])\n",
    "            weights = tf.transpose(weights_hid)\n",
    "            biases = tf.Variable(tf.zeros([size_after])+0.1, name='biases')\n",
    "            if size_after != int(layer_encoder_bin.get_shape()[1]):\n",
    "                layer_activations = tf.nn.relu(#sigmoid(\n",
    "                    tf.matmul(layer_activations, weights) + biases)\n",
    "            else:\n",
    "                layer_activations = tf.nn.sigmoid(\n",
    "                    tf.matmul(layer_activations, weights) + biases, \n",
    "                    name='decoder_bin')\n",
    "                layer_decoder_bin = layer_activations\n",
    "    \n",
    "    # the output decimal layer\n",
    "    #layer_decoder_bin = layer_activations\n",
    "    layer_activations = decode_binary(layer_decoder_bin)\n",
    "    \n",
    "    # extract a reference to the decoder\n",
    "    decoded = layer_activations\n",
    "    \n",
    "    return {\n",
    "        'encoded': encoded,\n",
    "        'decoded': decoded,\n",
    "        'cost_internal' : tf.sqrt(tf.reduce_mean(tf.square(input_features-decoded))), \n",
    "        'encoder_bin':layer_encoder_bin,\n",
    "        'decoder_bin':layer_decoder_bin\n",
    "            }\n",
    "\n",
    "\n",
    "def loss_mse(preds, targets):\n",
    "    mse = tf.sqrt(tf.reduce_mean(tf.square(preds-targets)), \n",
    "                  name='cost_mse')\n",
    "    return tf.reduce_mean(mse, name='cost_mse_mean')\n",
    "\n",
    "\n",
    "def loss_crossentropy_bin(preds_bin, targets_bin):\n",
    "    ce_terms = -1.0 * (tf.multiply(targets_bin, tf.log(preds_bin+1.0e-10)) + \\\n",
    "                      tf.multiply(1-targets_bin, tf.log(1-preds_bin+1.0e-10)))\n",
    "    #ce_terms = -1.0 * (tf.multiply(targets_bin, tf.log(tf.clip_by_value(preds_bin,1e-10,1.0))) + \\\n",
    "    #                  tf.multiply(1-targets_bin, tf.log(tf.clip_by_value(1-preds_bin,1e-10,1.0))))\n",
    "    ce_costs = decode_binary(ce_terms)\n",
    "    return tf.reduce_mean(ce_costs, name='cost_crossentropy_bin')\n",
    "\n",
    "\n",
    "def loss_mse_bin(preds_bin, targets_bin):\n",
    "    ce_terms = tf.pow(preds_bin-targets_bin, 2)\n",
    "    ce_costs = decode_binary(ce_terms)\n",
    "    return tf.reduce_mean(ce_costs, name='cost_mse_bin')\n",
    "\n",
    "\n",
    "def encode_binary_vect(input_vector, n_bits=N_BITS):\n",
    "    n_fe = int(input_vector.get_shape()[0])\n",
    "    # allocate memory\n",
    "    bitmat = []\n",
    "    for ind_bit in np.arange(0, n_bits, 1):\n",
    "        curr_bits = tf.floor(input_vector/2.0**(-ind_bit))\n",
    "        input_vector -= curr_bits * (2.0**(-ind_bit))\n",
    "        bitmat.append(tf.reshape(curr_bits,(-1,1)))\n",
    "    bitmat = tf.concat(bitmat, axis=1)\n",
    "    # return the result\n",
    "    encoded_vect = tf.reshape(bitmat,(-1,))\n",
    "    return encoded_vect\n",
    "\n",
    "\n",
    "def decode_binary_vect(input_vector, n_bits=N_BITS):\n",
    "    def decode_word(input_element, n_bits=n_bits):\n",
    "        base_weights = tf.constant(\n",
    "            2.0** np.arange(0,-n_bits,-1).astype(np.float32), shape=(n_bits,1))\n",
    "        curr_element = tf.matmul(tf.reshape(input_element,(1,N_BITS)),base_weights)\n",
    "        return curr_element\n",
    "        \n",
    "    n_fe = int(input_vector.get_shape()[0]) // n_bits\n",
    "    base_weights = tf.constant(\n",
    "            2.0** np.arange(0,-n_bits,-1).astype(np.float32), shape=(n_bits,1))\n",
    "    decoded_vector = tf.matmul(\n",
    "        tf.stack(tf.split(input_vector, n_fe)), base_weights)\n",
    "    decoded_vector = tf.stack(decoded_vector, axis=0)\n",
    "    return decoded_vector\n",
    "\n",
    "\n",
    "def encode_binary(input_matrix, n_bits=N_BITS):        \n",
    "    n_fe = int(input_matrix.get_shape()[1])\n",
    "    encoded_matrix = tf.map_fn(encode_binary_vect, input_matrix, \n",
    "                              parallel_iterations=n_bits)\n",
    "    return encoded_matrix\n",
    "\n",
    "\n",
    "def decode_binary(input_matrix, n_bits=N_BITS):        \n",
    "    n_fe = int(input_matrix.get_shape()[1]) // n_bits\n",
    "    decoded_matrix = tf.map_fn(decode_binary_vect, input_matrix, \n",
    "                               parallel_iterations=n_bits)\n",
    "    decoded_matrix = decoded_matrix[:,:,0]\n",
    "    return decoded_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create as tf session\n",
    "sess = tf.Session()\n",
    "\n",
    "# instantiate the graph and its inputs, etc.\n",
    "# place-holders\n",
    "ph_input_features = tf.placeholder(\"float\", [None, dataset_features.shape[1]])\n",
    "ph_input_features_full = tf.placeholder(\"float\", [None, dataset_features.shape[1]])\n",
    "ph_input_mask = tf.placeholder(\"float\", [None, dataset_features.shape[1]])\n",
    "ph_output_targets = tf.placeholder(\"float\", [None, dataset_targets.shape[1]])\n",
    "\n",
    "# net autoencoder\n",
    "nn_autoencoder = net_autoencoder_bin(\n",
    "    ph_input_features, size_layers_encoder=SIZE_LAYERS_ENCODER)\n",
    "\n",
    "if False:\n",
    "    nn_autoencoder_cost = loss_mse(preds=nn_autoencoder['decoded'], \n",
    "                                   targets=ph_input_features_full)\n",
    "elif True:\n",
    "    nn_autoencoder_cost = loss_crossentropy_bin(\n",
    "        preds_bin=nn_autoencoder['decoder_bin'], \n",
    "        targets_bin=encode_binary(ph_input_features_full))\n",
    "elif False:\n",
    "    nn_autoencoder_cost = loss_mse_bin(\n",
    "        preds_bin=nn_autoencoder['decoder_bin'], \n",
    "        targets_bin=encode_binary(ph_input_features_full))    \n",
    "\n",
    "\n",
    "# net predictor\n",
    "nn_predictor = net_mlp(nn_autoencoder['encoded'], \n",
    "                       SIZE_HIDDENS, dataset_targets.shape[1])\n",
    "nn_predictor_cost = loss_cross_entropy(preds=nn_predictor, labels=ph_output_targets)\n",
    "\n",
    "# create an optimizer\n",
    "train_step_ae = OPTIMIZER(learning_rate=LR_BASE_ENC).minimize(nn_autoencoder_cost)\n",
    "\n",
    "# initial the graph\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# summaries\n",
    "tf.summary.scalar('autoencoder_cost', nn_autoencoder_cost)\n",
    "summary_merged = tf.summary.merge_all()\n",
    "writer_train = tf.summary.FileWriter(DIR_SUMMARY + '/train',\n",
    "                                      sess.graph)\n",
    "# iterate and optimize\n",
    "iters_cnt = []\n",
    "costs_trn = []\n",
    "costs_val = []\n",
    "costs_tst = []\n",
    "cost_val_previous = 1.0e12\n",
    "iter_val_noimprove = 0\n",
    "\n",
    "for cnt_iter in range(ITERS_MAX_ENC):\n",
    "    # create feed data\n",
    "    feed_dict_trn = src.nn_training.feed_data(ph_input_features, ph_input_mask, \n",
    "                              ph_output_targets, ph_input_features_full,\n",
    "                              dataset_features, dataset_targets, \n",
    "                              phase='train', experiment=EXPERIMENT,\n",
    "                              size_batch=128, missing_portion = MISSING_PORTION, \n",
    "                              seed=None)\n",
    "    # do an optimization step\n",
    "    sess.run(train_step_ae, feed_dict=feed_dict_trn)\n",
    "    # each N iters calculate the train/validation/test costs\n",
    "    if cnt_iter%250 == 0:\n",
    "        summary = sess.run(summary_merged, feed_dict=feed_dict_trn)\n",
    "        writer_train.add_summary(summary, cnt_iter)\n",
    "        feed_dict_val = src.nn_training.feed_data(ph_input_features, ph_input_mask, \n",
    "                                  ph_output_targets, ph_input_features_full,\n",
    "                                  dataset_features, dataset_targets, \n",
    "                                  phase='validation', experiment=EXPERIMENT, \n",
    "                                  size_batch=2048, missing_portion = MISSING_PORTION, \n",
    "                                  seed=None)\n",
    "        feed_dict_tst = src.nn_training.feed_data(ph_input_features, ph_input_mask, \n",
    "                                  ph_output_targets, ph_input_features_full,\n",
    "                                  dataset_features, dataset_targets, \n",
    "                                  phase='test', experiment=EXPERIMENT, \n",
    "                                  size_batch=2048, missing_portion = MISSING_PORTION, \n",
    "                                  seed=None)\n",
    "        cost_trn = sess.run(nn_autoencoder_cost, feed_dict=feed_dict_trn)\n",
    "        cost_val = sess.run(nn_autoencoder_cost, feed_dict=feed_dict_val)\n",
    "        cost_tst = sess.run(nn_autoencoder_cost, feed_dict=feed_dict_tst)\n",
    "        iters_cnt.append(cnt_iter)\n",
    "        costs_trn.append(cost_trn)\n",
    "        costs_val.append(cost_val)\n",
    "        costs_tst.append(cost_tst)\n",
    "        print('cnt_iter: ' + str(cnt_iter) + ', cost_trn: ' + str(cost_trn)  + \\\n",
    "              ', cost_val: ' + str(cost_val) + ', cost_tst: ' + str(cost_tst), \n",
    "              end='\\r')\n",
    "        \n",
    "        # check early stop condition\n",
    "        if ITER_EARLYSTOP_ENC < iter_val_noimprove:\n",
    "            break\n",
    "        # if no improvement increase the counter\n",
    "        if cost_val > cost_val_previous:\n",
    "            iter_val_noimprove += 1\n",
    "        else:\n",
    "            iter_val_noimprove = 0\n",
    "        cost_val_previous = cost_val\n",
    "\n",
    "# save the encoder model\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, DIR_MODEL+'autoencoder')\n",
    "\n",
    "# report the missing reconstruction errors\n",
    "err_mis_base = feed_dict_tst[ph_input_features] - \\\n",
    "    feed_dict_tst[ph_input_features_full]\n",
    "err_mis_base = np.mean(err_mis_base**2.0)\n",
    "err_mis_rec = feed_dict_tst[ph_input_features_full] - \\\n",
    "    sess.run(nn_autoencoder['decoded'], feed_dict=feed_dict_tst)\n",
    "err_mis_rec = np.mean(err_mis_rec**2.0)\n",
    "print('')\n",
    "print('Error Missing Reconstruction Baseline: ' + str(err_mis_base))\n",
    "print('Error Missing Reconstruction MissingNet: ' + str(err_mis_rec))\n",
    "print('Error Reduction Rate: ' + str((err_mis_base-err_mis_rec)/err_mis_base))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(iters_cnt, costs_trn)\n",
    "plt.plot(iters_cnt, costs_val)\n",
    "plt.plot(iters_cnt, costs_tst)\n",
    "plt.legend(['costs_trn','costs_val','costs_tst'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_enc = LR_BASE_ENC\n",
    "lr_mlp = LR_BASE\n",
    "# create an optimizer\n",
    "vars_enc = []\n",
    "vars_mlp = []\n",
    "for var in tf.trainable_variables():\n",
    "    if var.name[:2] == 'AE':\n",
    "        vars_enc.append(var)\n",
    "    else:\n",
    "        vars_mlp.append(var)\n",
    "\n",
    "try:\n",
    "    train_step_enc = OPTIMIZER(learning_rate=lr_enc).minimize(\n",
    "        nn_predictor_cost, var_list=vars_enc)\n",
    "    train_step_mlp = OPTIMIZER(learning_rate=lr_mlp).minimize(\n",
    "        nn_predictor_cost, var_list=vars_mlp)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "if lr_enc != 0.0:\n",
    "    train_op = tf.group(train_step_enc, train_step_mlp)\n",
    "else:\n",
    "    train_op = train_step_mlp\n",
    "\n",
    "\n",
    "# initialize mlp vars\n",
    "initmlp_op = tf.variables_initializer(vars_mlp)\n",
    "sess.run(initmlp_op)\n",
    "\n",
    "# initialize uninitialized vars\n",
    "variables = tf.global_variables()\n",
    "init_flag = sess.run([tf.is_variable_initialized(v) for v in variables])\n",
    "vars_uninit = [v for v, f in zip(variables, init_flag) if not f]\n",
    "sess.run(tf.variables_initializer(vars_uninit))\n",
    "\n",
    "# iterate and optimize\n",
    "iters_cnt = []\n",
    "costs_trn = []\n",
    "costs_val = []\n",
    "costs_tst = []\n",
    "cost_val_previous = 1.0e12\n",
    "iter_val_noimprove = 0\n",
    "\n",
    "for cnt_iter in range(ITERS_MAX):\n",
    "    # create feed data\n",
    "    feed_dict_trn = src.nn_training.feed_data(ph_input_features, ph_input_mask, \n",
    "                              ph_output_targets, ph_input_features_full,\n",
    "                              dataset_features, dataset_targets, \n",
    "                              phase='train', experiment=EXPERIMENT,\n",
    "                              size_batch=128, missing_portion = MISSING_PORTION, \n",
    "                              seed=None)\n",
    "    # do an optimization step\n",
    "    sess.run(train_op, feed_dict=feed_dict_trn)\n",
    "    # each N iters calculate the train/validation/test costs\n",
    "    if cnt_iter%250 == 0:\n",
    "        feed_dict_val = src.nn_training.feed_data(ph_input_features, ph_input_mask, \n",
    "                              ph_output_targets, ph_input_features_full,\n",
    "                              dataset_features, dataset_targets, \n",
    "                              phase='validation', experiment=EXPERIMENT,\n",
    "                              size_batch=2048, missing_portion = MISSING_PORTION, \n",
    "                              seed=None)\n",
    "        feed_dict_tst = src.nn_training.feed_data(ph_input_features, ph_input_mask, \n",
    "                              ph_output_targets, ph_input_features_full,\n",
    "                              dataset_features, dataset_targets, \n",
    "                              phase='test', experiment=EXPERIMENT,\n",
    "                              size_batch=2048, missing_portion = MISSING_PORTION, \n",
    "                              seed=None)\n",
    "        cost_trn = sess.run(nn_predictor_cost, feed_dict=feed_dict_trn)\n",
    "        cost_val = sess.run(nn_predictor_cost, feed_dict=feed_dict_val)\n",
    "        cost_tst = sess.run(nn_predictor_cost, feed_dict=feed_dict_tst)\n",
    "        iters_cnt.append(cnt_iter)\n",
    "        costs_trn.append(cost_trn)\n",
    "        costs_val.append(cost_val)\n",
    "        costs_tst.append(cost_tst)\n",
    "        print('cnt_iter: ' + str(cnt_iter) + ', cost_trn: ' + str(cost_trn)  + \\\n",
    "              ', cost_val: ' + str(cost_val) + ', cost_tst: ' + str(cost_tst), \n",
    "              end='\\r')\n",
    "        \n",
    "        # check early stop condition\n",
    "        if ITER_EARLYSTOP < iter_val_noimprove:\n",
    "            break\n",
    "        # if no improvement increase the counter\n",
    "        if cost_val > cost_val_previous:\n",
    "            iter_val_noimprove += 1\n",
    "        else:\n",
    "            iter_val_noimprove = 0\n",
    "            cost_val_previous = cost_val\n",
    "\n",
    "# calculate the test accuracy\n",
    "preds_tst = sess.run(nn_predictor, feed_dict=feed_dict_tst)\n",
    "preds_tst = np.argmax(preds_tst, axis=1)\n",
    "accu_tst = np.mean(\n",
    "    np.argmax(feed_dict_tst[ph_output_targets], axis=1)==preds_tst)\n",
    "\n",
    "print('')\n",
    "print('The Accuracy of Test is: ' + str(accu_tst*100))\n",
    "plt.figure()\n",
    "plt.plot(iters_cnt, costs_trn)\n",
    "plt.plot(iters_cnt, costs_val)\n",
    "plt.plot(iters_cnt, costs_tst)\n",
    "plt.legend(['costs_trn','costs_val','costs_tst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Random and Sensitivity-based Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the trained autoencoder\n",
    "sess_enc = tf.Session()\n",
    "saver_enc = tf.train.import_meta_graph(DIR_MODEL+'autoencoder.meta')\n",
    "saver_enc.restore(sess_enc, tf.train.latest_checkpoint(DIR_MODEL))\n",
    "\n",
    "# deep copy\n",
    "feed_dict_trn = src.nn_training.feed_data(ph_input_features, ph_input_mask, \n",
    "                      ph_output_targets, ph_input_features_full,\n",
    "                      dataset_features, dataset_targets, \n",
    "                      phase='train', experiment=EXPERIMENT,\n",
    "                      size_batch=2048, missing_portion = 1.0, \n",
    "                      seed=None)\n",
    "feed_dict_tst = src.nn_training.feed_data(ph_input_features, ph_input_mask, \n",
    "                      ph_output_targets, ph_input_features_full,\n",
    "                      dataset_features, dataset_targets, \n",
    "                      phase='test', experiment=EXPERIMENT,\n",
    "                      size_batch=2048, missing_portion = 1.0, \n",
    "                      seed=None)\n",
    "feed_dict_rand = {k:v.copy() for k,v in feed_dict_tst.items()}\n",
    "feed_dict_muinfo = {k:v.copy() for k,v in feed_dict_tst.items()}\n",
    "feed_dict_sense = {k:v.copy() for k,v in feed_dict_tst.items()}\n",
    "\n",
    "n_tst = feed_dict_rand[ph_input_features_full].shape[0]\n",
    "n_fe = feed_dict_rand[ph_input_features_full].shape[1]\n",
    "\n",
    "accus_rand = []\n",
    "accus_muinfo = []\n",
    "accus_sense = []\n",
    "accus_base = []\n",
    "\n",
    "cost_rand = []\n",
    "cost_muinfo = []\n",
    "cost_sense = []\n",
    "cost_base = []\n",
    "\n",
    "# calculate the baseline accuracy\n",
    "preds_tst = sess.run(nn_predictor, feed_dict=feed_dict_tst)\n",
    "preds_tst = np.argmax(preds_tst, axis=1)\n",
    "accu_tst = 100*np.mean(\n",
    "    np.argmax(feed_dict_tst[ph_output_targets], axis=1)==preds_tst)\n",
    "accus_base.append(accu_tst)\n",
    "cost_base.append(0.0)\n",
    "\n",
    "# average performance of random selection\n",
    "for iter_randsel in range(RANDSEL_RUNS):\n",
    "    print('Random selection #'+str(iter_randsel), end='\\r')\n",
    "    # var init for each iter\n",
    "    accus_rand_iter = []\n",
    "    feed_dict_rand = {k:v.copy() for k,v in feed_dict_tst.items()}\n",
    "    # initial accuracy\n",
    "    preds_tst = sess.run(nn_predictor, feed_dict=feed_dict_rand)\n",
    "    preds_tst = np.argmax(preds_tst, axis=1)\n",
    "    accu_tst = 100*np.mean(\n",
    "    np.argmax(feed_dict_tst[ph_output_targets], axis=1)==preds_tst)\n",
    "    accus_rand_iter.append(accu_tst)\n",
    "    cost_rand.append(0.0)\n",
    "    # random sel\n",
    "    for iter_sel in range(n_fe):\n",
    "        for ind_tst in range(n_tst):\n",
    "            # rand sel alg\n",
    "            # select a sample\n",
    "            mask_tst = feed_dict_rand[ph_input_mask][ind_tst]\n",
    "            inds_missing = np.where(mask_tst==0)[0]\n",
    "            if len(inds_missing) > 0:\n",
    "                # query for feature\n",
    "                # rand sel alg\n",
    "                ind_sel_rand = np.random.choice(inds_missing)\n",
    "                features_tst = feed_dict_rand[ph_input_features][ind_tst]\n",
    "                feed_dict_rand[ph_input_features][ind_tst][ind_sel_rand] = \\\n",
    "                    feed_dict_rand[ph_input_features_full][ind_tst][ind_sel_rand]\n",
    "                feed_dict_rand[ph_input_mask][ind_tst][ind_sel_rand] = 1\n",
    "        # measure accuracy\n",
    "        preds_tst = sess.run(nn_predictor, feed_dict=feed_dict_rand)\n",
    "        preds_tst = np.argmax(preds_tst, axis=1)\n",
    "        accu_tst = 100*np.mean(\n",
    "            np.argmax(feed_dict_tst[ph_output_targets], axis=1)==preds_tst)\n",
    "        accus_rand_iter.append(accu_tst)\n",
    "        cost = np.sum(feed_dict_rand[ph_input_mask]*dataset_costs, axis=1).mean()\n",
    "        cost_rand.append(cost)\n",
    "    accus_rand.append(accus_rand_iter)\n",
    "    \n",
    "accus_rand = np.vstack(accus_rand).mean(axis=0)\n",
    "\n",
    "\n",
    "# performance of static mutual info selection\n",
    "print('')\n",
    "print('Static mutual info selection', end='\\r')\n",
    "preds_tst = sess.run(nn_predictor, feed_dict=feed_dict_muinfo)\n",
    "preds_tst = np.argmax(preds_tst, axis=1)\n",
    "accu_tst = 100*np.mean(\n",
    "    np.argmax(feed_dict_tst[ph_output_targets], axis=1)==preds_tst)\n",
    "accus_muinfo.append(accu_tst)\n",
    "cost_muinfo.append(0.0)\n",
    "\n",
    "if STATIC_ALG == 'MI':\n",
    "    #calculate MI\n",
    "    musel = sklearn.feature_selection.mutual_info_classif(\n",
    "       dataset_features[:,:], np.argmax(dataset_targets[:,:],1))\n",
    "    #musel, pval = sklearn.feature_selection.chi2(\n",
    "    #   dataset_features-dataset_features.min(0), np.argmax(dataset_targets,1))\n",
    "    #musel = np.arange(0, len(musel), 1, dtype=np.int)\n",
    "\n",
    "    #rfe = sklearn.feature_selection.RFE(sklearn.svm.SVR(kernel=\"linear\"), \n",
    "    #         1, step=1)\n",
    "    #rfe.fit(dataset_features[:1000], np.argmax(dataset_targets[:1000],1))\n",
    "    #musel = 1.0 / rfe.ranking_\n",
    "elif STATIC_ALG == 'SVM':\n",
    "    clf = sklearn.svm.SVC(kernel='linear')\n",
    "    clf.fit(dataset_features[:]/dataset_features.std(0), np.argmax(dataset_targets[:],1))\n",
    "    musel = np.abs(clf.coef_).sum(0)\n",
    "elif STATIC_ALG == 'none':\n",
    "    musel = None\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# do static feature sel \n",
    "for iter_sel in range(n_fe):\n",
    "    # check if we have to skip\n",
    "    if musel is None:\n",
    "        print('')\n",
    "        print('Static selection: SKIPPED')\n",
    "        break\n",
    "    else:\n",
    "        # normalize musel with cost values\n",
    "        feature_info = musel.copy()\n",
    "        musel /= dataset_costs\n",
    "    print('Static selection using: ' + STATIC_ALG + ', #FE: '+str(iter_sel), end='\\r')\n",
    "    # muinfo sel alg\n",
    "    for ind_tst in range(n_tst):\n",
    "        mask_tst = feed_dict_muinfo[ph_input_mask][ind_tst]\n",
    "        inds_missing = np.where(mask_tst==0)[0]\n",
    "        features_tst = feed_dict_muinfo[ph_input_features][ind_tst]\n",
    "        if len(inds_missing) > 0:\n",
    "            ind_sel_muinfo = inds_missing[np.argmax(musel[inds_missing])]\n",
    "            feed_dict_muinfo[ph_input_features][ind_tst][ind_sel_muinfo] = \\\n",
    "                feed_dict_muinfo[ph_input_features_full][ind_tst][ind_sel_muinfo]\n",
    "            feed_dict_muinfo[ph_input_mask][ind_tst][ind_sel_muinfo] = 1\n",
    "            \n",
    "\n",
    "    # calculate the muinfo accuracy\n",
    "    preds_tst = sess.run(nn_predictor, feed_dict=feed_dict_muinfo)\n",
    "    preds_tst = np.argmax(preds_tst, axis=1)\n",
    "    accu_tst = 100*np.mean(\n",
    "       np.argmax(feed_dict_tst[ph_output_targets], axis=1)==preds_tst)\n",
    "    accus_muinfo.append(accu_tst)\n",
    "    cost = np.sum(feed_dict_muinfo[ph_input_mask]*dataset_costs, axis=1).mean()\n",
    "    cost_muinfo.append(cost)\n",
    "print('')\n",
    "\n",
    "# performance of sens selection\n",
    "print('Sensitivity-based selection', end='\\r')\n",
    "preds_tst = sess.run(nn_predictor, feed_dict=feed_dict_sense)\n",
    "preds_tst = np.argmax(preds_tst, axis=1)\n",
    "accu_tst = 100*np.mean(\n",
    "    np.argmax(feed_dict_tst[ph_output_targets], axis=1)==preds_tst)\n",
    "accus_sense.append(accu_tst)\n",
    "cost_sense.append(0.0)\n",
    "\n",
    "# define the grad operation\n",
    "nn_predictor_prob = tf.nn.softmax(nn_predictor)\n",
    "op_grad = tf.abs(tf.gradients(nn_predictor_prob[:,0], \n",
    "                              nn_autoencoder['encoder_bin'])[0])\n",
    "for ind_y in range(1,int(nn_predictor_prob.get_shape()[1])):\n",
    "    op_grad = op_grad + tf.abs(\n",
    "        tf.gradients(nn_predictor_prob[:,ind_y], \n",
    "                     nn_autoencoder['encoder_bin'])[0])\n",
    "\n",
    "# probability estimation operation\n",
    "op_prob = nn_autoencoder['decoder_bin']\n",
    "# ground truth probabilities\n",
    "ph_prob_full = tf.placeholder(dtype=tf.float64, shape=(None,n_fe))\n",
    "op_prob_full = encode_binary(ph_prob_full)\n",
    "\n",
    "\"\"\"\n",
    "# sensitivity estimation operation\n",
    "#op_sens = tf.multiply(op_grad, op_prob)\n",
    "#op_sens = 1.0 - tf.abs(0.5 - tf.multiply(op_grad, op_prob))\n",
    "#op_sens = tf.reduce_sum(tf.reshape(op_sens, (-1,n_fe,8)), axis=2)\n",
    "\"\"\"\n",
    "\n",
    "#pdb.set_trace()\n",
    "\n",
    "# do feature sel\n",
    "ptime_start = time.process_time()\n",
    "selected_log = np.zeros((n_tst,n_fe), dtype=np.int) - 1 # -1 indicating invalid\n",
    "for iter_sel in range(n_fe):\n",
    "    #\n",
    "    print('Sensitivity-based selection, #FE: '+str(iter_sel), end='\\r')\n",
    "    # calc the sensitivities\n",
    "    #res_prob = sess.run(op_prob, feed_dict=feed_dict_sense)\n",
    "    \n",
    "    #res_prob = sess.run(op_prob_full, \n",
    "    #                    feed_dict={ph_prob_full:feed_dict_sense[ph_input_features_full]})\n",
    "    \n",
    "    res_prob = sess_enc.run(op_prob, feed_dict=feed_dict_sense)\n",
    "    \n",
    "    res_grad = sess.run(op_grad, feed_dict=feed_dict_sense)\n",
    "    res_sens = res_prob * res_grad\n",
    "    res_sens = np.sum(np.reshape(res_sens, (-1,n_fe,N_BITS)), axis=2) / \\\n",
    "        dataset_costs\n",
    "    \n",
    "    # sense sel alg\n",
    "    for ind_tst in range(n_tst):\n",
    "        mask_tst = feed_dict_sense[ph_input_mask][ind_tst]\n",
    "        inds_missing = np.where(mask_tst==0)[0]\n",
    "        features_tst = feed_dict_sense[ph_input_features][ind_tst]\n",
    "        grads_tst = res_sens[ind_tst]\n",
    "        if len(inds_missing) > 0:\n",
    "            ind_sel_sense = inds_missing[np.argmax(grads_tst[inds_missing])]\n",
    "            feed_dict_sense[ph_input_features][ind_tst][ind_sel_sense] = \\\n",
    "                feed_dict_sense[ph_input_features_full][ind_tst][ind_sel_sense]\n",
    "            feed_dict_sense[ph_input_mask][ind_tst][ind_sel_sense] = 1\n",
    "            # log it\n",
    "            selected_log[ind_tst,iter_sel] = ind_sel_sense\n",
    "            \n",
    "\n",
    "    # append the test accuracy\n",
    "    accus_base.append(accus_base[-1])\n",
    "    cost_base.append(cost_rand[-1])\n",
    "    # calculate the sens accuracy\n",
    "    preds_tst = sess.run(nn_predictor, feed_dict=feed_dict_sense)\n",
    "    preds_tst = np.argmax(preds_tst, axis=1)\n",
    "    accu_tst = 100*np.mean(\n",
    "       np.argmax(feed_dict_tst[ph_output_targets], axis=1)==preds_tst)\n",
    "    accus_sense.append(accu_tst)\n",
    "    cost = np.sum(feed_dict_sense[ph_input_mask]*dataset_costs, axis=1).mean()\n",
    "    cost_sense.append(cost)\n",
    "    \n",
    "ptime_end = time.process_time()\n",
    "print('')\n",
    "print('Dataset :', DATASET)\n",
    "print('Processing-time per sample: ' + \\\n",
    "      str(1000.0 / n_tst *(ptime_end - ptime_start)) + ' (ms)')\n",
    "\n",
    "auc_rand = np.sum(np.array(accus_rand) / 100.0) / len(accus_rand)\n",
    "auc_muinfo = np.sum(np.array(accus_muinfo) / 100.0) / len(accus_muinfo)\n",
    "auc_sense = np.sum(np.array(accus_sense) / 100.0) / len(accus_sense)\n",
    "acc_th = accus_sense[-1] * 0.95\n",
    "ind_th = np.where(np.array(accus_sense) > acc_th)[0][0]\n",
    "auc_rand_th = np.mean(np.array(accus_rand)[:ind_th] / 100.0) \n",
    "auc_muinfo_th = np.mean(np.array(accus_muinfo)[:ind_th] / 100.0) \n",
    "auc_sense_th = np.mean(np.array(accus_sense)[:ind_th] / 100.0) \n",
    "\n",
    "\n",
    "print('')\n",
    "print('AUC Rand: ' + str(auc_rand))\n",
    "print('AUC Static: ' + str(auc_muinfo))\n",
    "print('AUC Sense: ' + str(auc_sense))\n",
    "print('')\n",
    "print('AUC Rand (th): ' + str(auc_rand_th))\n",
    "print('AUC Static (th): ' + str(auc_muinfo_th))\n",
    "print('AUC Sense (th): ' + str(auc_sense_th))\n",
    "print('')\n",
    "print('Accuracy at 0% : ' + str(accus_sense[0]))\n",
    "print('Accuracy at 25% : ' + str(accus_sense[int(0.25*len(accus_sense))]))\n",
    "print('Accuracy at 50% : ' + str(accus_sense[int(0.50*len(accus_sense))]))\n",
    "print('Accuracy at 75% : ' + str(accus_sense[int(0.75*len(accus_sense))]))\n",
    "print('Accuracy at 100% : ' + str(accus_sense[-1]))\n",
    "\n",
    "cost_total = cost_sense[-1]\n",
    "ind_25 = np.where(cost_sense > cost_total*0.25)[0][0]\n",
    "ind_50 = np.where(cost_sense > cost_total*0.50)[0][0]\n",
    "ind_75 = np.where(cost_sense > cost_total*0.75)[0][0]\n",
    "\n",
    "print('')\n",
    "print('Accuracy at 0%   total cost: ' + str(accus_sense[0]))\n",
    "print('Accuracy at 25%  total cost: ' + str(accus_sense[ind_25]))\n",
    "print('Accuracy at 50%  total cost: ' + str(accus_sense[ind_50]))\n",
    "print('Accuracy at 75%  total cost: ' + str(accus_sense[ind_75]))\n",
    "print('Accuracy at 100% total cost: ' + str(accus_sense[-1]))\n",
    "\n",
    "# plot the results\n",
    "plt.figure()\n",
    "plt.plot(accus_base)\n",
    "plt.plot(accus_rand)\n",
    "plt.plot(accus_muinfo)\n",
    "plt.plot(accus_sense)\n",
    "plt.legend(['Base', 'RandSel', 'Static', 'FACT'])\n",
    "plt.xlabel('Features Acquired')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('./run_outputs/learn_curve_' + DATASET + '.pdf')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(cost_base, accus_base)\n",
    "plt.plot(cost_rand, accus_rand)\n",
    "plt.plot(cost_muinfo, accus_muinfo)\n",
    "plt.plot(cost_sense, accus_sense)\n",
    "plt.legend(['Base', 'RandSel', 'Static', 'FACT'])\n",
    "plt.xlabel('Acquisition Cost')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('./run_outputs/cost_curve_' + DATASET + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
